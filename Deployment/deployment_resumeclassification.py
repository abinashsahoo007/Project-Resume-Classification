# -*- coding: utf-8 -*-
"""Deployment_ResumeClassification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s_ud0cUSMy1ZwSJoqBB8i-T-oTe0RRu1

# **Deployment**

# **Streamlit App**
"""

#! pip install streamlit
#! pip install joblib
#! pip install scikit-learn==0.24.2
#! pip install --upgrade scikit-learn
#! pip install scikit-learn==1.2.2
#! pip install docx2txt
#! pip install pdfplumber

import numpy as np
import pandas as pd
import streamlit as st
import joblib
import pickle
import re
import os
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer
from nltk.stem import WordNetLemmatizer
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

import en_core_web_sm
nlp=en_core_web_sm.load()

st.title('RESUME CLASSIFICATION ANALYSIS')
st.subheader('Welcome to Resume Classification App')

def extract_skills(resume_text):
  nlp_text=nlp(resume_text)
  noun_chunks=nlp_text.noun_chunks
  tokens=[token.text for token in nlp_text if not token.is_stop] # remove stopwords and implement tokenization

  data=pd.read_csv('skills.csv')  #reading csv file
  skills=list(data.columns.values) #extract values
  skillset=[]

  for token in tokens:                  #checking one-gram
    if token.lower() in skills:
      skillset.append(token)

  for token in noun_chunks:             #checking for bi-grams and tri-grams
    token=token.text.lower().strip()
    if token in skills:
      skillset.append(token)

  return[i.capitalize() for i in set([i.lower() for i in skillset])]
  #first convert all tokens in lower case, and then apply set in order to remove duplicates and keep only unique values
  #then perform capitalize so change first letter capital and remaining letter remain small in every token

import docx2txt
import pdfplumber

def gettext(filename):
  fulltext = '' #create empty string

  if filename.type=="application/vnd.openxmlformats-officedocument.wordprocessingml.document" and filename.name.endswith('.docx'):
    # process DOCX file
    fulltext=docx2txt.process(filename)

  elif filename.type=="application/pdf" and filename.name.endswith('pdf'):
    #process PDF file
    with pdfplumber.open(filename) as pdf_file:
      for page in pdf_file.pages:
        fulltext=fulltext+page.extract_text()  # Extract text from each page and append to fulltext

  return fulltext

def display(doc_file):
  resume=[]

  try:
    if doc_file.type=="application/vnd.openxmlformats-officedocument.wordprocessingml.document":
      resume.append(docx2txt.process(doc_file))
    elif doc_file.type=="application/pdf":
      with pdfplumber.open(doc_file) as pdf:
        for page in pdf.pages:
          resume.append(page.extract_text())
    else:
      raise ValueError('Unsupported file type')


  except Exception as e:
    print(f'An error occured: {e}')
    resume.append("Failed to process the document.")

  return resume

# def func for preprocessing of data
def preprocess(sentence):
  sentence=str(sentence)               # convert the data in string
  sentence=sentence.lower()            # convert all the letter in the sentence into lowercase
  sentence=sentence.replace("{html}","") # replace html sen to white space
  cleanr=re.compile("<.*?>")             # access all the html tags
  cleantext=re.sub(cleanr,'',sentence)   # find all the html tags and convert it into empty space
  rem_url=re.sub(r'https\S+','',cleantext)  # remove all url's  sentence
  rem_num=re.sub('[0-9]+', '', rem_url)     # remove all the number from sentence
  tokenizer=RegexpTokenizer(r'\w+')         # implementing tokenization in order to sep all words into one word in sen
  tokens=tokenizer.tokenize(rem_num)        # applying tokenizer on rem_num
  filtered_words=[w for w in tokens if len(w)>2 if not w in stopwords.words('english')] #remove stopwords
  lemmatizer=WordNetLemmatizer()         #implementing lemmatizer
  lemma_words=[lemmatizer.lemmatize(w) for w in filtered_words]  # applying lemmatization in filter words
  return " ".join(lemma_words)

# streamlit code
#loading models

#loading randomforest model
final_model=open(r'final_rf_model.pkl',mode='rb')
model=pickle.load(final_model)

#loading tfidf model
tfidf_model=open(r'tfidf_model.pkl',mode='rb')
tfidf=pickle.load(tfidf_model)

# upload function
upload_files=st.file_uploader('Upload Your Resumes',type=['docx','pdf'],accept_multiple_files=True)

file_type=pd.DataFrame([],columns=['Uploaded File','Predicted Profile','Skills'])
filename=[]
predicted=[]
skills=[]

for doc_file in upload_files:
  if doc_file is not None:
    name,ext=os.path.splitext(doc_file.name)
    filename.append(name)
    cleaned=preprocess(display(doc_file))
    vector=tfidf.transform([cleaned])
    result=model.predict(vector)[0]
    predicted.append(result)
    extracting_text=gettext(doc_file)
    skills.append(extract_skills(extracting_text))

if len(predicted) > 0:
  file_type['Uploaded File']=filename
  file_type['Predicted Profile']=predicted
  file_type['Skills']=skills
  st.table(file_type.style.format())

select=['PeopleSoft','SQL Developer','React Developer','Workday']
st.subheader('Select Resumes as per Requirement')
options=st.selectbox('Fields',select)

if options=='PeopleSoft':
  st.table(file_type[file_type['Predicted Profile'] == 'PeopleSoft'])
elif options=='SQL Developer':
  st.table(file_type[file_type['Predicted Profile'] == 'SQL Developer'])
elif options=='React Developer':
  st.table(file_type[file_type['Predicted Profile'] == 'React Developer'])
elif options=='Workday':
  st.table(file_type[file_type['Predicted Profile'] == 'Workday'])



